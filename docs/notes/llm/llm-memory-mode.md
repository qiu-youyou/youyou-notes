---
tag:
  - LLM

description: 本文介绍了大语言模型（LLM）实现记忆功能的常见模式，包括缓冲记忆、窗口记忆、Token 限制、摘要总结、混合记忆和向量记忆等 。内容涵盖各类记忆系统的原理、优缺点及适用场景。

# date: 2025/10/21 09:37:45
---

# LLM 实现记忆功能的常见模式

本文介绍了大语言模型 `(LLM) `实现记忆功能的常见模式，包括缓冲记忆、窗口记忆、Token 限制、摘要总结、混合记忆和向量记忆等。内容涵盖各类记忆系统的原理、优缺点及适用场景。

> 一个能记住你的大模型，才是真正的智能体。

## 📙 LLM 的记忆系统

在与大语言模型 `(LLM, Large Language Model)` 交互时，
我们常常会觉得它能 “记住” 之前的对话，比如当你说 “我昨天告诉过你我的名字” 时，它能准确地回答出来。

> 但实际上 `LLM` 并不真正拥有记忆, 它并不会真正地记得任何事情。

`LLM` 在生成回复时，只依赖于它 “看到” 的输入文本 (也就是 `Prompt`)。
它不会保存历史状态，也不会在不同会话之间共享上下文。
一旦输入被清空，模型就彻底 “失忆” 了。

要让模型具备 “记忆” 能力，我们就必须在它外部构建一个记忆系统 `(Memory System)`。

通过 `保存 → 检索 → 注入` 的方式，让模型 “看起来” 能记住过去。

> 所以记忆的本质就是：让模型在下一次生成时看到过去的信息。

## 📙 记忆系统的基本工作原理

为 `LLM` 添加记忆其实非常简单，你只需要在每次请求中，额外携带上一轮或多轮的历史对话内容。

具体就是在 `Prompt` 中预留 `chat_history ` 占位符，将  `Human/Ai`  的历史对话信息插入到占位符中，
并且实时保存  `Human/Ai`  的对话信息，在每一次对话时插入到预留占位符,
就可以完成最简单的记忆功能。

::: info 例如：

系统提示：
你是一个友好的 AI 助手。

[对话记录]

用户：你好，我叫小林，喜欢拍照。

AI：你好小林！摄影真是个美妙的爱好。

[本轮提问]

用户：我上次说我喜欢什么？

:::

模型看到完整上下文后，就能推断出答案：

> “你上次说你喜欢拍照。”

这就是最基础的 “上下文记忆 `(Contextual Memory)`”。
它的效果虽然不错，但问题也很明显：
当对话变长、内容增多时，`Prompt` 会变得又长又贵，性能迅速下降。

所以，工程上通常会采用更高效、更灵活的记忆策略。

## 📙 LLM 常见记忆模式与实现思路

虽然直接插入历史对话最简单，但并不是最优方案。
当对话变得越来越长，`LLM` 的上下文窗口很快会被填满。
这时，我们就需要用不同的 “记忆模式” 来平衡性能、成本与记忆深度。

不同的记忆模式有不同的适用场景，以下是常见的几种实现方式 👇

### 缓冲记忆

> 将所有历史对话完整保存，每次都注入到 `Prompt` 。

最基础的记忆模式，将所有  `Human/Ai`  生成的消息全部存储起来，
每次需要使用时将保存的所有聊天消息列表传递到 `Prompt` 中，
通过往用户的输入中添加历史对话信息/记忆，可以让 `LLM` 能理解之前的对话内容。

::: tip 优点：

信息完整无损：模型能看到完整对话历史，不会丢失任何上下文信息，理解最连贯。

实现简单：只需将历史消息数组追加到输入即可，几乎所有大模型接口都支持这种形式。

易于调试：便于分析模型是如何基于上下文生成答案的，透明度高。

:::

::: warning 缺点：

性能消耗高：随着历史增长，Token 数成倍增加，响应速度变慢且成本上升。

上下文受限：当历史过长超过模型最大 Token 限制时，早期对话会被截断，导致“遗忘”。

不适合长期对话：对需要长期交互的智能体而言，缓冲记忆难以扩展。

:::

类比：就像人类 “短期记忆”，能记得刚刚发生的所有细节，但装不下太多东西。

适用场景：小规模会话、演示、`Prompt` 调试、短时任务型机器人。

### 缓冲窗口记忆

> 只保留最近 N 轮对话内容（或 N 条消息）。

缓冲窗口记忆只保存最近的几次 `Human/Ai`  生成的消息，
它基于 [`缓冲记忆`](llm-memory-mode#缓冲记忆) 思想，并添加了一个窗口值，
这意味着只保留一定数量的过去互动，然后“忘记”之前的互动。

::: tip 优点：

- 控制 Token 数量：窗口机制可以限制上下文长度，避免提示词过长造成性能问题。
- 适配小模型：对上下文窗口较短的模型（如 4K Token 模型）特别友好。
- 实现简洁：相比摘要或向量存储，无需额外模块或数据库即可完成。

:::

::: warning 缺点：

- 记忆范围有限：只保留最近几轮内容，超出窗口的早期信息会被 “遗忘”。
- 上下文割裂：当用户提到早期话题时，模型可能无法关联，出现 “前后不连贯” 的回答。
- 难以持久保存：窗口一旦滑动，内容即被抛弃，不适合长期交互应用。

:::

适用场景：日常聊天机器人、任务助手等。

### Token 缓冲记忆

> 限制记忆的 `Token` 总数，例如最多保留 2000 个 `Token`，而不是固定轮数。

与 [`缓冲窗口记忆`](llm-memory-mode#缓冲窗口记忆) 类似，但不是限制 “对话轮数”，而是限制 “最大 `Token` 数”。
当历史内容的 `Token` 总量超过阈值时，系统自动删除最早的部分。

::: tip 优点：

- 控制更精确：比固定轮数的窗口更灵活，可以精细控制 Token 成本。
- 性能稳定：始终保持模型输入长度可控，响应时间恒定。
- 通用性强：几乎所有大模型都能兼容这种策略。

:::

::: warning 缺点：

- 记忆有限：仍然会遗忘早期内容，无法长期追踪用户状态。
- 实现复杂度：需要额外计算 Token 长度（不同模型 Token 计数规则不同）。
- 上下文变化快：长文本会被优先裁剪，导致记忆跳跃。

:::

适用场景：高性能实时系统、语音助手、低延迟在线问答服务。

### 摘要总结记忆

> 当对话累计到一定长度时，将早期内容自动总结成摘要，替代原文加入 `Prompt` 。

举个例子：原始对话包含 `50` 轮内容，系统通过一个中间 `LLM` 或规则，生成：

`《用户是一名设计师，喜欢户外摄影，目前正在准备去新疆旅行。》`

之后每次调用模型时，只注入这段摘要，而非全部历史文本。

::: tip 优点：

- 支持超长对话：对话可以持续增长，摘要能以极低成本表示历史信息。
- 节省 Token 成本：通过摘要压缩上下文，显著减少输入长度。
- 兼顾长期理解：模型能记住用户的长期偏好或身份特征。
- 具备语义记忆特征：记得“事实”而非“原文”。

:::

::: warning 缺点：

- 细节损失：摘要往往保留核心语义，丢失细节或语气信息。
- 依赖摘要质量：摘要 LLM 若性能不佳，会导致信息偏差或错误。
- 额外成本：每次更新摘要都需额外一次模型调用。

:::

类比： 就像人类的 “长期记忆” 会忘掉细节，但记得重要事实。

适用场景：个人助理、长期陪伴类 AI、学习型机器人、AI 角色扮演系统。

### 混合记忆

> 结合 “摘要总结记忆 + 窗口记忆”：近期对话保留原文，旧对话保存摘要。

摘要缓冲混合记忆结合了  [摘要总结记忆](llm-memory-mode#摘要总结记忆)  与  [缓冲窗口记忆](llm-memory-mode#缓冲窗口记忆)，

它对 对话进行摘要总结，同时保留最近互动中的原始内容，
但不是简单地清除旧的交互，而是将它们编译成摘要并同时使用，并且使用标记长度而不是交互数量来确定何时清除交互。

::: tip 优点：

- 兼顾长期与短期记忆：长期为模糊记忆，短期为精准记忆。
- Token 成本可控：摘要部分节省空间，窗口部分保证即时语义。
- 体验自然：用户会感觉模型“既记得过去，也懂现在”。
- 具备成长性：模型能通过不断更新摘要实现“经验积累”。

:::

::: warning 缺点：

- 实现复杂：需要设计摘要生成与合并策略。
- 摘要误差影响长期表现：一旦摘要偏差，会长期污染记忆，长期互动的内容仍然为模糊记忆。。
- 计算成本略高：既要维持窗口，又要定期摘要。

:::

适用场景： AI 助手、教育类智能体、企业级对话系统。

### 向量记忆

> 把每段对话或事件转换为语义向量 `(Embedding)`，
> 存入向量数据库 `(如 Faiss、Milvus、Pinecone )` 等。
> 当用户再次提问时，通过语义相似度检索最相关的内容，再注入 `Prompt`。

大概流程就是 把 `用户输入 → 转换为向量 ，检索前 K 条最相似历史，注入 Prompt → 生成响应` 。
这类记忆模式能记住所有内容，在细节部分比摘要总结要强，但是比缓冲记忆弱，消耗 `Token` 方面相对平衡。

::: tip 优点：

- 语义级记忆：能理解相似含义的不同表达，例如“我想去云南旅游” ≈ “我计划去昆明”。
- 无限容量：理论上可记住所有历史，只需足够存储空间。
- Token 成本低：只在需要时加载相关记忆，而非全部历史。
- 可跨主题检索：支持非线性对话、跨时间关联。

:::

::: warning 缺点：

- 依赖外部系统：需要额外的 Embedding 模型和向量数据库。
- 性能瓶颈：大规模检索会引入延迟。
- 召回质量不稳定：检索策略不佳时会出现记忆“偏题”。
- 开发复杂度高：涉及存储管理、向量更新、重排序等。

:::

类比： 像人类通过 “意义” 回忆事件，而不是死记细节。

适用场景：知识问答系统、长期 AI 助手、企业文档问答、AI Agent 系统。

## 📙 记忆系统的基本流程

在一个支持记忆的 `LLM` 系统中，信息流大致如下：

```
用户输入
   ↓
[Embedding 编码] → 存入向量数据库
   ↓
新问题 → 语义检索 → 召回最相关的记忆
   ↓
构建新的 Prompt（加入记忆内容）
   ↓
LLM 生成响应
   ↓
更新记忆数据库

```

这与 `RAG（检索增强生成）` 模型架构类似。

区别在于：`RAG` 召回 “外部知识” ，而 `Memory` 召回 “用户经历”。

## 📙 记忆层次化设计

为了让模型更像 有思维 的个体，记忆往往被分层管理：

| 层次     | 功能               | 示例                 |
| -------- | ------------------ | -------------------- |
| 短期记忆 | 保留当前对话上下文 | 用户刚刚问了天气     |
| 长期记忆 | 保存用户画像和偏好 | 用户喜欢户外活动     |
| 语义记忆 | 总结概念和知识     | 晴天更适合拍照       |
| 反思记忆 | 自我学习与调整     | 用户希望我回答更简洁 |

层次化记忆让 `AI` 拥有 “人格”、“成长感” 和 “连续性”。

这是从 `Chatbot` 到真正 `Agent` 的关键一步。

## 📙 选择合适的方案

| 应用场景             | 推荐记忆类型        | 说明                             |
| -------------------- | ------------------- | -------------------------------- |
| 聊天机器人           | 缓冲窗口记忆        | 结构简单、响应快，适合轻量场景。 |
| 长期陪伴型助手       | 混合记忆            | 同时拥有短期对话与长期用户画像。 |
| 企业知识问答 / Agent | 向量记忆            | 语义检索能力强，能跨主题理解。   |
| 实时语音助手         | 令牌缓冲记忆        | 控制性能与延迟。                 |
| 教育型 AI 教师       | 摘要记忆 + 反思机制 | 适合持续学习与成长型场景。       |

## 📙 总结

记忆并非只是让模型 记得信息，
更是赋予它 理解时间、形成人格、持续学习 的能力。

通过设计合理的记忆机制，
一个 `LLM` 可以从 即时回答问题 升级为 理解用户、持续进化 的智能体。

> 记忆，让语言模型从能说话，变成会思考。
