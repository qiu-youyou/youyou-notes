---
tag:
  - LLM
tags:
  - LangChain

description: LangChain æ ¸å¿ƒç»„ä»¶ä¸ç”¨æ³•ï¼ŒåŒ…æ‹¬ Promptã€Messageã€Modelã€OutputParser åŠ LCEL/Runnableã€‚é€šè¿‡ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä»æç¤ºè®¾è®¡ã€æ¨¡å‹è°ƒç”¨åˆ°ç»“æœè§£æï¼Œå†åˆ°é“¾å¼ç»„åˆï¼Œä»¥å¿«é€Ÿä¸Šæ‰‹å¹¶æ„å»ºå¯æ‰©å±•çš„ LLM åº”ç”¨ã€‚

date: 2025-09-29 15:12:41
sticky: 9998
---

# ä¸€æ¯å’–å•¡çš„æ—¶é—´ ä¸Šæ‰‹ LangChain

`LangChain` æ ¸å¿ƒç»„ä»¶ä¸ç”¨æ³•ï¼ŒåŒ…æ‹¬ `Prompt`ã€`Message`ã€`Model`ã€`OutputParser` åŠ `LCEL/Runnable`ã€‚é€šè¿‡ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä»æç¤ºè®¾è®¡ã€æ¨¡å‹è°ƒç”¨åˆ°ç»“æœè§£æï¼Œå†åˆ°é“¾å¼ç»„åˆï¼Œä»¥å¿«é€Ÿä¸Šæ‰‹å¹¶æ„å»ºå¯æ‰©å±•çš„ `LLM` åº”ç”¨ã€‚

## ğŸŒµ LangChain æ˜¯ä»€ä¹ˆ

`LangChain` æ˜¯ä¸€ä¸ª é¢å‘å¤§è¯­è¨€æ¨¡å‹ `ï¼ˆLLMï¼‰` çš„åº”ç”¨å¼€å‘æ¡†æ¶ï¼Œå®ƒçš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š

ğŸ‘‰ è®©å¼€å‘è€…æ›´å®¹æ˜“åœ°ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶æŠŠå®ƒä»¬èå…¥åˆ°å®é™…åº”ç”¨ä¸­ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦ LangChain

å¤§è¯­è¨€æ¨¡å‹ï¼ˆæ¯”å¦‚ `GPT-4ã€Claudeã€Llama`ï¼‰å¾ˆå¼ºå¤§ï¼Œä½†å®ƒä»¬æœ¬è´¨ä¸Šåªæ˜¯â€œè¾“å…¥ â†’ è¾“å‡ºâ€çš„é»‘ç®±ï¼š

ä½ è¾“å…¥ä¸€æ®µæ–‡æœ¬ï¼Œå®ƒè¿”å›ä¸€æ®µæ–‡æœ¬ã€‚
å®ƒå¹¶ä¸çŸ¥é“ä½ è¦ç”¨å®ƒåšä»€ä¹ˆï¼Œä¹Ÿä¸ä¼šä¸»åŠ¨çº¦æŸè¾“å‡ºæ ¼å¼ã€‚

åœ¨å®é™…å¼€å‘ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°é—®é¢˜ï¼š

```
Prompt ç®¡ç†å›°éš¾ï¼šæ¨¡å‹çš„è¡¨ç°é«˜åº¦ä¾èµ–äºæç¤ºè¯ï¼Œå¦‚ä½•ç»„ç»‡å’Œå¤ç”¨ Promptï¼Ÿ
è¾“å‡ºä¸å¯æ§ï¼šæ¨¡å‹å¯èƒ½è¿”å›ä¹±ä¸ƒå…«ç³Ÿçš„å†…å®¹ï¼Œæ€ä¹ˆè§£ææˆç»“æ„åŒ–ç»“æœï¼Ÿ
å¤šæ­¥éª¤é€»è¾‘ï¼šå¾ˆå¤šåº”ç”¨éœ€è¦å¤šæ¬¡è°ƒç”¨æ¨¡å‹ï¼Œæ€ä¹ˆæŠŠæ­¥éª¤ç»„ç»‡èµ·æ¥ï¼Ÿ
é›†æˆå¤–éƒ¨å·¥å…·ï¼šæ¨¡å‹éœ€è¦è°ƒç”¨æ•°æ®åº“ã€æœç´¢å¼•æ“æˆ– APIï¼Œè¯¥æ€ä¹ˆæ¡¥æ¥ï¼Ÿ
è°ƒè¯•å’Œç›‘æ§ï¼šæ€ä¹ˆè®°å½•è°ƒç”¨è¿‡ç¨‹ã€ç›‘æ§ token æ¶ˆè€—ã€å¯è§†åŒ–è°ƒè¯•ï¼Ÿ
```

> LangChain çš„è®¾è®¡åˆè¡·å°±æ˜¯ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ã€‚

#### LangChain çš„æ ¸å¿ƒä»·å€¼

```
æŠ½è±¡ï¼šæŠŠ Promptã€Modelã€Parserã€Chain ç­‰æŠ½è±¡æˆæ¨¡å—ã€‚
å¯ç»„åˆï¼šæ‰€æœ‰æ¨¡å—å®ç°ç»Ÿä¸€æ¥å£ (Runnable)ï¼Œå¯ä»¥åƒç§¯æœ¨ä¸€æ ·ç»„åˆã€‚
å¯æ‰©å±•ï¼šæ”¯æŒä¸åŒçš„æ¨¡å‹ (OpenAIã€Anthropicã€æœ¬åœ° LLM)ã€ä¸åŒçš„å­˜å‚¨ã€ä¸åŒçš„å·¥å…·ã€‚
å¯è§‚æµ‹ï¼šå†…ç½®å›è°ƒ (Callback) æœºåˆ¶ï¼Œæ”¯æŒç›‘æ§ã€æ—¥å¿—å’Œæµå¼è¾“å‡ºã€‚
```

> LangChain å°±æ˜¯ä¸€ä¸ªå¸®ä½ æŠŠ LLM å˜æˆâ€œå¯ç”¨åº”ç”¨â€çš„å¼€å‘æ¡†æ¶ã€‚

## ğŸŒµ Prompt åŸºç¡€ç”¨æ³•

#### ä»€ä¹ˆæ˜¯ Prompt

`Prompt`ï¼ˆæç¤ºè¯ï¼‰å°±æ˜¯ä½ å‘Šè¯‰å¤§è¯­è¨€æ¨¡å‹è¯¥åšä»€ä¹ˆçš„æŒ‡ä»¤ã€‚

å®ƒå¯ä»¥æ˜¯ç®€å•çš„ä¸€å¥è¯ï¼š

::: code-group

```md [] {}
è¯·æŠŠè¿™å¥è¯ç¿»è¯‘æˆè‹±æ–‡ï¼šä½ å¥½ï¼Œä¸–ç•Œ
```

:::

ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªç»“æ„åŒ–çš„å¯¹è¯ï¼š

::: code-group

```md [] {}
System: ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç¿»è¯‘
User: è¯·æŠŠä¸‹é¢çš„å¥å­ç¿»è¯‘æˆè‹±æ–‡ï¼šä½ å¥½ï¼Œä¸–ç•Œ
```

:::

#### Prompt çš„ç»„æˆ

åœ¨ LangChain ä¸­ï¼Œ`Prompt` è¢«è®¾è®¡æˆä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å—ï¼Œä¾¿äºç®¡ç†ã€å¤ç”¨å’ŒåŠ¨æ€ç”Ÿæˆã€‚

LangChain æŠŠ `Prompt` è§†ä¸ºç”± æ¨¡æ¿ + å˜é‡ ç»„æˆï¼š

- æ¨¡æ¿ (Template)ï¼šå­—ç¬¦ä¸²æˆ–å¯¹è¯ç»“æ„ï¼Œé‡Œé¢åŒ…å«å ä½ç¬¦ã€‚
- å˜é‡ (Variables)ï¼šè¿è¡Œæ—¶ä¼ å…¥çš„å€¼ï¼Œå¡«å……åˆ°æ¨¡æ¿é‡Œã€‚

::: code-group

```python [] {}
from LangChain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("è¯·ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}")
prompt_value = prompt.format(text="ä½ å¥½")
print(prompt_value)
# -> è¯·ç¿»è¯‘æˆè‹±æ–‡ï¼šä½ å¥½

```

:::

#### Prompt çš„å­ç»„ä»¶

å¯¹äº `Prompt Template`ï¼Œ åœ¨ `LangChain` ä¸­ï¼Œåˆæ¶µç›–äº†å¤šä¸ªå­ç»„ä»¶ï¼Œ
ä¾‹å¦‚ï¼šè§’è‰²æç¤ºæ¨¡æ¿ã€æ¶ˆæ¯å ä½ç¬¦ã€æ–‡æœ¬æç¤ºæ¨¡æ¿ã€èŠå¤©æ¶ˆæ¯æç¤ºæ¨¡æ¿ã€æç¤ºã€æ¶ˆæ¯ç­‰:

```
PromptTemplate ï¼šåˆ›å»ºæ–‡æœ¬æ¶ˆæ¯æç¤ºæ¨¡æ¿ï¼Œç”¨äºä¸å¤§è¯­è¨€æ¨¡å‹/æ–‡æœ¬ç”Ÿæˆæ¨¡å‹è¿›è¡Œäº¤äº’ã€‚
ChatPromptTemplate ï¼šç”¨äºåˆ›å»ºèŠå¤©æ¶ˆæ¯æç¤ºæ¨¡æ¿ï¼Œä¸€èˆ¬ç”¨äºä¸èŠå¤©æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚
MessagePlaceholder ï¼šæ¶ˆæ¯å ä½ç¬¦ï¼Œåœ¨èŠå¤©æ¨¡å‹ä¸­å¯¹ä¸ç¡®å®šæ˜¯å¦éœ€è¦çš„æ¶ˆæ¯è¿›è¡Œå ä½ã€‚
SystemMessagePromptTemplate ï¼šç”¨äºåˆ›å»ºç³»ç»Ÿæ¶ˆæ¯æç¤ºæ¨¡æ¿ï¼Œè§’è‰²ä¸ºç³»ç»Ÿã€‚
HumanMessagePromptTemplate ï¼šç”¨äºåˆ›å»ºäººç±»æ¶ˆæ¯æç¤ºæ¨¡æ¿ï¼Œè§’è‰²ä¸ºäººç±»ã€‚
AIMessagePromptTemplate ï¼šç”¨äºåˆ›å»º AI æ¶ˆæ¯æç¤ºæ¨¡æ¿ï¼Œè§’è‰²ä¸º AIã€‚
PipelinePromptTemplate ï¼šç”¨äºåˆ›å»ºç®¡é“æ¶ˆæ¯ï¼Œå¯ä»¥å°†æç¤ºæ¨¡æ¿ä½œä¸ºå˜é‡è¿›è¡Œå¿«é€Ÿå¤ç”¨ã€‚
```

Prompt çš„å¸¸ç”¨æ–¹æ³•åŠåŠŸèƒ½:

```
partialï¼šç”¨äºæ ¼å¼åŒ–æç¤ºæ¨¡æ¿ä¸­çš„éƒ¨åˆ†å˜é‡ã€‚
formatï¼šä¼ é€’å˜é‡æ•°æ®ï¼Œæ ¼å¼åŒ–æç¤ºæ¨¡æ¿ä¸ºæ–‡æœ¬æ¶ˆæ¯ã€‚
invokeï¼šä¼ é€’å˜é‡æ•°æ®ï¼Œæ ¼å¼åŒ–æç¤ºæ¨¡æ¿ä¸ºæç¤ºã€‚
to_stringï¼šå°†æç¤º/æ¶ˆæ¯æç¤ºåˆ—è¡¨è½¬æ¢æˆå­—ç¬¦ä¸²ã€‚
to_messagesï¼šç”¨äºå°†æç¤ºè½¬æ¢æˆæ¶ˆæ¯åˆ—è¡¨ã€‚
```

#### Prompt åŸºç¡€ç”¨æ³•

#### PromptTemplate

> é€‚åˆéå¯¹è¯ä»»åŠ¡ï¼ˆç¿»è¯‘ã€æ€»ç»“ã€åˆ†ç±»ï¼‰ç­‰å•è½®ä»»åŠ¡ã€‚

::: code-group

```python [] {}
from LangChain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("è¯·ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}")
prompt_value = prompt.invoke(text="ä½ å¥½")
print(prompt.format(text="ä½ å¥½"))
print(prompt_value.to_string())
print(prompt_value.to_messages())

```

---

:::

#### ChatPromptTemplate & MessagesPlaceholder

> å¦‚æœä½ è¦åšå¤šè½®å¯¹è¯ï¼Œè¿˜å¯ä»¥é€šè¿‡ `MessagesPlaceholder` æ¥æ’å…¥åŠ¨æ€å†å²ã€‚

::: code-group

```python [] {}
from LangChain.prompts import MessagesPlaceholder, HumanMessagePromptTemplate
from LangChain_core.messages import AIMessage
from LangChain_core.prompts import ChatPromptTemplate

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªèŠå¤©åŠ©æ‰‹"),
    # è°ƒç”¨æ—¶ä½ å¯ä»¥ä¼ å…¥ historyï¼Œè¿™æ ·å°±èƒ½ä¿ç•™ä¸Šä¸‹æ–‡ã€‚
    MessagesPlaceholder("history"),
    HumanMessagePromptTemplate.from_template("è¯·ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}"),
])

chat_prompt_value = chat_prompt.invoke({
    "chat_history": [
        ("human", "æˆ‘å«å°æ˜"),
        AIMessage("ä½ å¥½ï¼Œæˆ‘æ˜¯ChatGPTï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°æ‚¨"),
    ],
    "text": "ä½ å¥½",
})
print(chat_prompt_value)

```

:::

## ğŸŒµ Message åŸºç¡€ç”¨æ³•

#### ä¸ºä»€ä¹ˆéœ€è¦ Message

åœ¨ `LLM (Language Model)` ä¸­ï¼Œè¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯ å­—ç¬¦ä¸²,

è€Œåœ¨ `ChatModel (å¯¹è¯æ¨¡å‹)` ä¸­ï¼Œæ¨¡å‹çš„è¾“å…¥è¾“å‡ºéµå¾ª å¯¹è¯æ ¼å¼ã€‚

å¦‚æœåªç”¨ `str` ï¼Œæ¨¡å‹æ— æ³•åˆ†è¾¨è¿™æ®µè¯æ˜¯è°è¯´çš„ï¼Œè€Œåœ¨ `LangChain` ä¸­ `Message` æ˜¯æ¶ˆæ¯ç»„ä»¶ï¼Œ
å¹¶ä¸”æ‰€æœ‰æ¶ˆæ¯éƒ½å…·æœ‰: `type(ç±»å‹)ã€content(å†…å®¹)ã€response_metadata(å“åº”å…ƒæ•°æ®)`ã€‚

> è¿™æ ·ï¼Œæ¨¡å‹ä¸ä»…èƒ½ç†è§£è¾“å…¥å†…å®¹ï¼Œè¿˜èƒ½ç†è§£ä¸Šä¸‹æ–‡å’Œèº«ä»½è®¾å®šã€‚

#### Message åŸºç¡€ç”¨æ³•

LangChain å°è£…çš„ `Message` æ¶µç›–äº† 5 ç§ç±»å‹çš„æ¶ˆæ¯ï¼š
`SystemMessageã€HumanMessageã€AIMessageã€FunctionMessageã€ToolMessage`ã€‚

åœ¨å®è·µä¸­ï¼Œé€šå¸¸ç”¨ `ChatPromptTemplate` ç”Ÿæˆ `Message`ï¼Œå†äº¤ç»™ `ChatModel`:

#### è®¾å®šè§’è‰²:

::: code-group

```python [] {}
from LangChain.prompts import ChatPromptTemplate

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½{role}"),
    ("user", "è¯·ç”¨{lang}è§£é‡Šï¼š{topic}")
])

messages = chat_prompt.format_messages(role="ç§‘å­¦å®¶", lang="ä¸­æ–‡", topic="é‡å­çº ç¼ ")

print(messages)

# è¾“å‡º
"""
[
  SystemMessage(content="ä½ æ˜¯ä¸€ä½ç§‘å­¦å®¶"),
  HumanMessage(content="è¯·ç”¨ä¸­æ–‡è§£é‡Šï¼šé‡å­çº ç¼ ")
]
"""

```

:::

#### ä¿æŒä¸Šä¸‹æ–‡å¯¹è¯

::: code-group

```python [] {}
history = [
    HumanMessage(content="ä½ å¥½"),
    AIMessage(content="ä½ å¥½ï¼æˆ‘æ˜¯ AI åŠ©æ‰‹ã€‚"),
    HumanMessage(content="ç»™æˆ‘è®²ä¸ªç¬‘è¯")
]
chat.invoke(history)

```

:::

## ğŸŒµ Model ç»„ä»¶åŸºç¡€ç”¨æ³•

#### ä»€ä¹ˆæ˜¯ Model

åœ¨ LangChain é‡Œï¼Œ`Model` å°±æ˜¯å’Œå¤§è¯­è¨€æ¨¡å‹äº¤äº’çš„æ¥å£ã€‚

- ä½ ç»™å®ƒè¾“å…¥ `(Prompt)`ï¼Œå®ƒè¿”å›è¾“å‡ºï¼ˆæ–‡æœ¬æˆ–æ¶ˆæ¯ï¼‰ã€‚
- LangChain æŠŠä¸åŒå‚å•†çš„æ¨¡å‹ `(OpenAIã€Anthropicã€LLaMA)` ç»Ÿä¸€æˆäº†ç›¸ä¼¼çš„æ¥å£ã€‚
- æ‰€æœ‰æ¨¡å‹éƒ½å®ç°äº† `Runnable` åè®®ï¼Œæ‰€ä»¥è°ƒç”¨æ–¹å¼ä¸€è‡´ `(.invoke / .batch / .stream)`ã€‚

> è¿™æ ·ä¸€æ¥ï¼Œä½ çš„åº”ç”¨å°±ä¸ç”¨ä¾èµ–æŸä¸€ä¸ªå…·ä½“æ¨¡å‹ï¼Œè€Œæ˜¯å¯ä»¥éšæ—¶åˆ‡æ¢ã€‚

#### Model çš„åˆ†ç±»

LangChain é‡Œå¸¸è§çš„æ¨¡å‹ä¸»è¦æœ‰ä¸¤ç±»ï¼š

- LLMï¼ˆLanguage Modelï¼‰: è¾“å…¥è¾“å‡ºéƒ½æ˜¯çº¯æ–‡æœ¬å­—ç¬¦ä¸²ã€é€‚åˆå•è®ºä»»åŠ¡ï¼ˆç¿»è¯‘ã€æ€»ç»“ï¼‰
- ChatModelï¼ˆå¯¹è¯æ¨¡å‹ï¼‰: è¾“å…¥è¾“å…¥å¯¹è¯æ¶ˆæ¯ã€é€‚åˆå¤šè½®å¯¹è¯ã€èŠå¤©æœºå™¨äººã€‚

> LLM è¾“å…¥è¾“å‡ºæ˜¯çº¯å­—ç¬¦ä¸²ï¼ŒChatModel è¾“å…¥è¾“å‡ºæ˜¯ Message åˆ—è¡¨.

#### åŸºç¡€è°ƒç”¨æ–¹å¼

#### å•æ¬¡è°ƒç”¨ï¼ˆinvokeï¼‰

> ä¼ é€’å¯¹åº”çš„æ–‡æœ¬æç¤º/æ¶ˆæ¯æç¤ºï¼Œå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¯¹åº”çš„å†…å®¹ã€‚

::: code-group

```python [] {}
from LangChain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from LangChain_openai import OpenAI, ChatOpenAI

# LLM è°ƒç”¨
llm = OpenAI(model="text-davinci-003")
print(llm.invoke("ç»™æˆ‘ä¸€å¥åŠ±å¿—æ ¼è¨€"))

# ChatModel è°ƒç”¨ï¼ˆå­—ç¬¦ä¸²ï¼‰
chat = ChatOpenAI(model="gpt-3.5-turbo")
print(chat.invoke("ç»™æˆ‘ä¸€å¥åŠ±å¿—æ ¼è¨€"))

# ChatModel è°ƒç”¨ï¼ˆMessage åˆ—è¡¨ï¼‰
# ç¼–æ’æç¤ºè¯
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªèŠå¤©åŠ©æ‰‹"),
    HumanMessagePromptTemplate.from_template("{query}"),
])

# åˆ›å»ºå¤§è¯­è¨€æ¨¡å‹
llm = ChatOpenAI(model="gpt-3.5-turbo")

ai_message = llm.invoke(prompt.invoke({"query": "ä½ å¥½"}))

print(ai_message.type)
print(ai_message.content)
print(ai_message.response_metadata)
```

:::

#### æ‰¹é‡è°ƒç”¨ï¼ˆbatchï¼‰:

> æ‰¹é‡è°ƒç”¨å¯ä»¥èŠ‚çœæ—¶é—´ï¼Œé€‚åˆä¸€æ¬¡å¤„ç†å¤šæ¡æ•°æ®ï¼ˆå¦‚æ‰¹é‡ç¿»è¯‘ã€æ–‡æœ¬åˆ†ç±»ï¼‰ã€‚

::: code-group

```python [] {}
ai_messages = llm.batch([
    prompt.invoke({"query": "å†™ä¸€ä¸ªå…³äºæ˜¥å¤©çš„æ¯”å–»å¥"}),
    prompt.invoke({"query": "å†™ä¸€ä¸ªå…³äºå†¬å¤©çš„æ¯”å–»å¥"}),
])

for ai_message in ai_messages:
    print(ai_message.content)

```

:::

#### æµå¼è°ƒç”¨ï¼ˆstreamï¼‰:

> æµå¼è¾“å‡ºç‰ˆæœ¬ é€‚åˆéœ€è¦è¾¹ç”Ÿæˆè¾¹å±•ç¤º çš„åœºæ™¯ï¼ˆæ¯”å¦‚èŠå¤©æœºå™¨äººï¼‰ã€‚

::: code-group

```python [] {}
response = llm.stream(prompt.invoke({"query": "å†™ä¸€ä¸ª50å­—çš„çŸ­æ•…äº‹"}))

for chunk in response:
    print(chunk.content, flush=True, end="")

```

:::

## ğŸŒµ OutputParser åŸºç¡€ç”¨æ³•

#### OutputParser æ˜¯ä»€ä¹ˆ

`LangChain` é‡Œçš„ `OutputParser`(è¾“å‡ºè§£æå™¨) å°±æ˜¯ï¼š

æŠŠæ¨¡å‹ç”Ÿæˆçš„ è‡ªç”±æ–‡æœ¬ è½¬æ¢æˆ ç»“æ„åŒ–ç»“æœï¼ˆJSONã€åˆ—è¡¨ã€æ•°å­—ã€Pydantic å¯¹è±¡ç­‰ï¼‰ã€‚

å› ä¸ºå¤§æ¨¡å‹çš„è¾“å‡ºå¾€å¾€æ˜¯è‡ªç„¶è¯­è¨€ï¼Œè€Œåœ¨åº”ç”¨é‡Œæˆ‘ä»¬éœ€è¦ å¯æ§æ ¼å¼ï¼š

å¦‚ï¼šæ•°æ®æå–ï¼ˆè¿”å› JSONï¼‰ã€åˆ†ç±»ä»»åŠ¡ï¼ˆè¿”å›æ ‡ç­¾ï¼‰ã€æ•°å€¼è®¡ç®—ï¼ˆè¿”å›æ•°å­—ï¼‰ã€ç¨‹åºè°ƒç”¨ï¼ˆè¿”å›ç»“æ„åŒ–å‚æ•°ï¼‰ç­‰ã€‚

> `OutputParser` æ˜¯æ¨¡å‹è¾“å‡ºåˆ°ç¨‹åºå¯ç”¨æ•°æ®çš„æ¡¥æ¢

#### OutputParser æ ¸å¿ƒä½œç”¨

- å°† è‡ªç”±æ–‡æœ¬ æˆ– æ¨¡å‹ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€ è½¬æˆ å¯ç¨‹åºåŒ–å¤„ç†çš„æ•°æ®ã€‚
- ä½¿å¾—æ¨¡å‹ç”Ÿæˆç»“æœæ›´ç¨³å®šã€å¯é ã€‚
- æé«˜ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚æ•°æ®åº“å­˜å‚¨ã€API è°ƒç”¨ã€å‰ç«¯å±•ç¤ºï¼‰çš„å¯æ§æ€§ã€‚

#### å¸¸è§ OutputParser

åœ¨ `LangChain` ä¸­é¢„è®¾äº†å¤§é‡çš„è¾“å‡ºè§£æå™¨ï¼Œ
è¾“å‡ºè§£æå™¨é€šå¸¸åŒ…å«ä¸¤ä¸ªæŠ½è±¡å‡½æ•°çš„å®ç°ï¼Œè¿™ä¹Ÿæ˜¯è‡ªå®šä¹‰è¾“å‡ºè§£æå™¨éœ€è¦å®ç°çš„ä¸¤ä¸ªå‡½æ•°ï¼š

- `get_format_instructions`ï¼šç”¨æ¥çº¦å®šè¾“å‡ºçš„æ ¼å¼ï¼Œå¹¶è½¬æ¢ä¸ºæè¿°æ–‡æœ¬ã€‚
- `parse`ï¼šç”¨æ¥è§£æ LLM çš„è¾“å‡ºä¸ºçº¦å®šçš„æ ¼å¼ã€‚

#### StrOutputParse

> è¿™æ˜¯æœ€åŸºç¡€çš„è§£æå™¨ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²ã€‚

::: code-group

```python [] {}
from LangChain_core.output_parsers import StrOutputParser
from LangChain_core.prompts import ChatPromptTemplate
from LangChain_openai import ChatOpenAI

# 1.ç¼–æ’æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_template("{query}")

# 2.æ„å»ºå¤§è¯­è¨€æ¨¡å‹
llm = ChatOpenAI(model="gpt-3.5-turbo-16k")

# 3.åˆ›å»ºå­—ç¬¦ä¸²è¾“å‡ºè§£æå™¨
parser = StrOutputParser()

# 4.è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç»“æœå¹¶è§£æ
content = parser.invoke(llm.invoke(prompt.invoke({"query": "ä½ å¥½ï¼Œä½ æ˜¯?"})))

print(content)

```

:::

#### PydanticOutputParser

> æŒ‰å­—æ®µè§£æ Pydantic å¯¹è±¡

::: code-group

```python [] {}
from LangChain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

class Person(BaseModel):
    name: str = Field(..., description="äººçš„åå­—")
    age: int = Field(..., description="äººçš„å¹´é¾„")

parser = PydanticOutputParser(pydantic_object=Person)
parsed = parser.invoke('{"name": "å¼ ä¸‰", "age": 25}')
print(parsed)
# è¾“å‡º: Person(name='å¼ ä¸‰', age=25)

```

:::

#### PydanticOutputParser / JsonOutputParser

> - PydanticOutputParser â†’ è¿”å› Pydantic å¯¹è±¡ï¼Œå¸¦ç±»å‹éªŒè¯
> - JsonOutputParser â†’ è¿”å›å­—å…¸ï¼Œç›´æ¥è§£æ JSON

::: code-group

```python [] {}
from LangChain_core.output_parsers import JsonOutputParser
from LangChain_core.prompts import ChatPromptTemplate
from LangChain_core.pydantic_v1 import BaseModel, Field
from LangChain_openai import ChatOpenAI


# 1. å®šä¹‰ Pydantic æ•°æ®ç»“æ„
class WeatherReport(BaseModel):
    location: str = Field(description="åŸå¸‚åç§°")
    temperature: str = Field(description="å½“å‰æ¸©åº¦")
    condition: str = Field(description="å¤©æ°”çŠ¶å†µï¼Œæ¯”å¦‚æ™´å¤©ã€å¤šäº‘ã€ä¸‹é›¨ç­‰")

parser = JsonOutputParser(pydantic_object=WeatherReport)

# 2. æ„å»ºæç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_template(
    "è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„åŸå¸‚ç”Ÿæˆå¤©æ°”æŠ¥å‘Šã€‚\n{format_instructions}\nç”¨æˆ·è¾“å…¥: {query}"
).partial(format_instructions=parser.get_format_instructions())

# 3. æ„å»ºå¤§è¯­è¨€æ¨¡å‹
llm = ChatOpenAI(model="gpt-3.5-turbo-16k")

# 4. è°ƒç”¨æ¨¡å‹å¹¶è§£æè¾“å‡º
weather_report = parser.invoke(llm.invoke(prompt.invoke({"query": "åŒ—äº¬"})))

# 5. è¾“å‡ºç»“æœ
print(type(weather_report))  # <class 'dict'> æˆ– Pydantic æ¨¡å‹
print("åŸå¸‚:", weather_report.get("location"))
print("æ¸©åº¦:", weather_report.get("temperature"))
print("å¤©æ°”çŠ¶å†µ:", weather_report.get("condition"))
print(weather_report)

```

:::

## ğŸŒµ LCEL ä¸ Runnable åè®®

ç»“åˆä¸Šé¢çš„å†…å®¹ï¼Œæˆ‘ä»¬ç”¨åˆ°çš„ `Prompt`ã€`Model`ã€`OutputParser` æœ¬è´¨éƒ½æ˜¯ `Runnable`ã€‚

è¿™äº›ç»„ä»¶æœ‰ä¸€ä¸ªå…±åŒçš„è°ƒç”¨ï¼š`invoke`ï¼Œå¹¶ä¸”æ¯ä¸€ä¸ªç»„ä»¶çš„è¾“å‡ºéƒ½æ˜¯ä¸‹ä¸€ä¸ªç»„ä»¶çš„è¾“å…¥:

::: code-group

```python [] {}
prompt = ChatPromptTemplate.from_template("{query}")
llm = ChatOpenAI(model="gpt-3.5-turbo-16k")
parser = StrOutputParser()

output = parser.invoke(llm.invoke(prompt.invoke({"query": "ä½ å¥½"})))
```

:::

```
1. PromptTemplateï¼šè¾“å…¥å­—å…¸ â†’ è¾“å‡º prompt æ¶ˆæ¯
2. ChatModelï¼šè¾“å…¥æ¶ˆæ¯ â†’ è¾“å‡º AIMessage
3. OutputParserï¼šè¾“å…¥ AIMessage â†’ è¾“å‡ºç»“æ„åŒ–æ•°æ®
```

è¿™ç§å†™æ³•è™½ç„¶èƒ½å®ç°å¯¹åº”çš„åŠŸèƒ½ï¼Œä½†æ˜¯å­˜åœ¨å¾ˆå¤šç¼ºé™·ï¼š

```
åµŒå¥—å¼å†™æ³•è®©ç¨‹åºçš„ç»´æŠ¤æ€§ä¸å¯é˜…è¯»æ€§å¤§å¤§é™ä½ï¼Œå½“éœ€è¦ä¿®æ”¹æŸä¸ªç»„ä»¶æ—¶ï¼Œå˜å¾—å¼‚å¸¸å›°éš¾ã€‚
æ²¡æ³•å¾—çŸ¥æ¯ä¸€æ­¥çš„å…·ä½“ç»“æœä¸æ‰§è¡Œè¿›åº¦ï¼Œå‡ºé”™æ—¶éš¾ä»¥æ’æŸ¥ã€‚
åµŒå¥—å¼å†™æ³•æ²¡æ³•é›†æˆå¤§é‡çš„ç»„ä»¶ï¼Œç»„ä»¶è¶Šæ¥è¶Šå¤šæ—¶ï¼Œä»£ç ä¼šå˜æˆâ€œä¸€æ¬¡æ€§â€ä»£ç ã€‚
```

> ä¸å‰ç«¯ä»£ç ä¸­çš„ å›è°ƒåœ°ç‹± éå¸¸ç›¸ä¼¼

#### LCEL è¡¨è¾¾å¼ä¸ Runnable å¯è¿è¡Œåè®®

#### Runnable = LangChain ä¸­çš„â€œç»Ÿä¸€å¯è¿è¡Œå•å…ƒâ€æ¥å£ã€‚

> ä»»ä½•æ¨¡å‹ã€è§£æå™¨ã€å‡½æ•°ã€æˆ–ç»„åˆç®¡é“éƒ½å¯ä»¥è¢«åŒ…è£…æˆ Runnableï¼Œ
> æä¾›ç»Ÿä¸€çš„è°ƒç”¨å¥‘çº¦ï¼ˆinvoke / batch / stream ç­‰æ–¹æ³•ï¼‰ï¼Œä»è€Œå®ç°å¯ç»„åˆã€å¯å¹¶è¡Œã€å¯æµå¼çš„å·¥ä½œæµã€‚

#### LCEL = åŸºäº Runnable çš„å£°æ˜å¼ç¼–ç¨‹/ç¼–æ’å±‚ï¼Œ

> å…è®¸ä½ ç”¨æ›´ç®€æ´çš„è¡¨è¾¾å¼ï¼ˆæˆ–æ„å»º Runnables çš„åŸè¯­ï¼‰æ¥ç»„åˆã€é…ç½®ã€å¹¶è¡ŒåŒ–å’Œéƒ¨ç½²é“¾å¼å·¥ä½œæµã€‚
> å®ƒæ˜¯é¢å‘â€œæè¿°ä»»åŠ¡è¦åšä»€ä¹ˆâ€çš„è¯­æ³•ç³–/ç¼–æ’å±‚ï¼ŒèƒŒåè¿è¡Œæ—¶ä¼šæŠŠè¿™äº›æè¿°ç¼–è¯‘ä¸ºå¯æ‰§è¡Œçš„ Runnablesã€‚

LCEL è®©ä½ å†™å‡ºç±»ä¼¼ `prompt | model | parser` è¿™ç§ `é“¾å¼è¡¨è¾¾å¼ï¼ˆchainï¼‰`ï¼Œ
è€Œä¸æ˜¯ä¸€å † `.invoke(...)` åµŒå¥—ã€‚

> Prompt/Model/Parser æ˜¯ç –å—ï¼ŒRunnable æ˜¯æ ‡å‡†æ¥å£ï¼ŒLCEL æ˜¯æ­ç§¯æœ¨çš„è¯­è¨€ã€‚

#### æŠŠ Prompt â†’ ChatModel â†’ OutputParser ä¸²æˆä¸€ä¸ª LCEL Pipeline

::: code-group

```python [] {}
# 1.æ„å»ºç»„ä»¶
from LangChain_core.output_parsers import JsonOutputParser
from LangChain_core.prompts import ChatPromptTemplate
from LangChain_core.pydantic_v1 import BaseModel, Field
from LangChain_openai import ChatOpenAI

# 1. å®šä¹‰è¾“å‡ºç»“æ„
class TravelPlan(BaseModel):
    city: str = Field(description="æ¨èçš„åŸå¸‚")
    reason: str = Field(description="æ¨èçš„ç†ç”±")
    must_try_food: str = Field(description="å¿…å°çš„ç‰¹è‰²ç¾é£Ÿ")

parser = JsonOutputParser(pydantic_object=TravelPlan)

# 2. æ„å»º Promptï¼ˆæ³¨æ„ partial æ³¨å…¥ format_instructionsï¼‰
prompt = ChatPromptTemplate.from_template(
    "è¯·æ ¹æ®ç”¨æˆ·çš„åå¥½æ¨èä¸€ä¸ªæ—…æ¸¸ç›®çš„åœ°ã€‚\n"
    "{format_instructions}\n"
    "ç”¨æˆ·åå¥½: {preference}"
).partial(format_instructions=parser.get_format_instructions())

# 3. åˆ›å»ºæ¨¡å‹
llm = ChatOpenAI(model="gpt-3.5-turbo")

# 4. chainï¼šPrompt â†’ Model â†’ Parser
chain = prompt | llm | parser

# 5. è°ƒç”¨
result = chain.invoke({"preference": "å–œæ¬¢æµ·è¾¹å’Œç¾é£Ÿ"})

print(result)
# è¾“å‡º
"""
{
  "city": "å·´å¡ç½—é‚£",
  "reason": "æ‹¥æœ‰ç¾ä¸½çš„åœ°ä¸­æµ·æµ·å²¸çº¿å’Œä¸°å¯Œçš„æ–‡åŒ–æ°›å›´",
  "must_try_food": "æµ·é²œé¥­ï¼ˆPaellaï¼‰"
}
"""

```

:::

#### RunnableParallel å¹¶è¡Œè¿è¡Œ

`RunnableParallel` æ˜¯ LangChain ä¸­å°è£…çš„æ”¯æŒè¿è¡Œå¤šä¸ª `Runnable` çš„ç±»
ï¼Œä¸€èˆ¬ç”¨äºæ“ä½œ `Runnable` çš„è¾“å‡ºï¼Œä»¥åŒ¹é…åºåˆ—ä¸­ä¸‹ä¸€ä¸ª `Runnable` çš„è¾“å…¥ï¼Œèµ·åˆ°å¹¶è¡Œè¿è¡Œ `Runnable` å¹¶æ ¼å¼åŒ–è¾“å‡ºç»“æ„çš„ä½œç”¨ã€‚

ä¾‹å¦‚ `RunnableParallel` å¯ä»¥è®©æˆ‘ä»¬åŒæ—¶æ‰§è¡Œå¤šæ¡ `Chain`ï¼Œ
ç„¶åä»¥å­—å…¸çš„å½¢å¼è¿”å›å„ä¸ª `Chain` çš„ç»“æœï¼Œå¯¹æ¯”æ¯ä¸€æ¡é“¾å•ç‹¬æ‰§è¡Œï¼Œæ•ˆç‡ä¼šé«˜å¾ˆå¤š:

ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œå‡è®¾æˆ‘ä»¬è¦åŒæ—¶ç”Ÿæˆ æ—…æ¸¸è®¡åˆ’ å’Œ ä¸€å¥å®£ä¼ æ–‡æ¡ˆï¼Œå°±å¯ä»¥è¿™æ ·ï¼š

::: code-group

```python [] {}
from LangChain_core.runnables import RunnableParallel

tagline_prompt = ChatPromptTemplate.from_template("è¯·ä¸º {city} å†™ä¸€å¥æ—…æ¸¸å®£ä¼ æ ‡è¯­")
tagline_chain = tagline_prompt | llm

parallel_chain = RunnableParallel({
    "plan": chain,       # ä¸Šé¢å®šä¹‰çš„ plan é“¾
    "tagline": tagline_chain
})

result = parallel_chain.invoke({"preference": "å–œæ¬¢æµ·è¾¹å’Œç¾é£Ÿ", "city": "å·´å¡ç½—é‚£"})
print(result)
# è¾“å‡º
"""
{
  "plan": {"city": "å·´å¡ç½—é‚£", "reason": "...", "must_try_food": "..."},
  "tagline": "å·´å¡ç½—é‚£ï¼šé˜³å…‰ä¸æµ·æµªçš„å‘³è§‰ç››å®´"
}
"""

```

:::

#### RunnablePassthrough ç®€åŒ– invoke

`RunnablePassthrough` é¡¾åæ€ä¹‰ï¼šç›´æ¥æŠŠè¾“å…¥åŸæ ·ä¼ é€’ä¸‹å»ï¼Œä¸åšä»»ä½•å¤„ç†ã€‚

åœ¨ `LCEL` ç»„åˆä¸­ï¼Œå¦‚æœä½ éœ€è¦æŠŠè¾“å…¥â€œç›´æ¥å¸¦ä¸‹å»â€ç»™æŸä¸ªç¯èŠ‚ï¼Œå¯ä»¥ç”¨å®ƒæ¥ç®€åŒ–ã€‚
ç®€å•ç†è§£ï¼šå®ƒå°±æ˜¯ä¸€ä¸ªâ€œä»€ä¹ˆéƒ½ä¸å¹²çš„ Runnableâ€ã€‚
å¯ä»¥åœ¨å¹¶è¡Œï¼ˆRunnableParallelï¼‰é‡Œï¼Œæ—¢æƒ³ä¿ç•™åŸå§‹è¾“å…¥ï¼Œåˆæƒ³è·‘å…¶ä»–é“¾ã€‚

> æ’ç­‰æ˜ å°„ï¼šè¾“å…¥å•¥ â†’ è¾“å‡ºå•¥

ç¤ºä¾‹æ¨¡æ‹Ÿä¸€ä¸ªæ£€ç´¢å™¨åŠŸèƒ½, è¿™æ˜¯ RAG æ¨¡å¼çš„å…¸å‹ç”¨æ³•:

è¿™é‡Œå®šä¹‰äº†ä¸¤ä¸ªå˜é‡ï¼š`context` å’Œ `query`ã€‚
æ¨¡å‹ç”Ÿæˆç­”æ¡ˆæ—¶ï¼Œä¼šæŠŠã€ˆæ£€ç´¢åˆ°çš„ `context` ä¸ ç”¨æˆ·çš„ `query`ã€‰ ä¸€èµ·ä¼ å…¥ã€‚

::: code-group

```python [] {}
from LangChain_core.output_parsers import StrOutputParser
from LangChain_core.prompts import ChatPromptTemplate
from LangChain_core.runnables import RunnablePassthrough
from LangChain_openai import ChatOpenAI

# çœŸå®åœºæ™¯é‡Œï¼Œè¿™ä¸€æ­¥ä¼šå»è°ƒç”¨ å‘é‡æ•°æ®åº“ã€çŸ¥è¯†åº“ APIï¼Œè¿”å›ç›¸å…³ä¸Šä¸‹æ–‡ã€‚
def retrieval(query: str) -> str:
    """ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ£€ç´¢å™¨å‡½æ•°"""
    print("æ­£åœ¨æ£€ç´¢:", query)
    return "æˆ‘æ˜¯AI åŠ©æ‰‹"

# 1.ç¼–æ’prompt
# è¿™é‡Œå®šä¹‰äº†ä¸¤ä¸ªå˜é‡ï¼šcontext å’Œ queryã€‚
# æ¨¡å‹ç”Ÿæˆç­”æ¡ˆæ—¶ï¼Œä¼šæŠŠæ£€ç´¢åˆ°çš„ context ä¸ç”¨æˆ· query ä¸€èµ·ä¼ å…¥ã€‚
prompt = ChatPromptTemplate.from_template("""è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å›ç­”ï¼Œå¯ä»¥å‚è€ƒå¯¹åº”çš„ä¸Šä¸‹æ–‡è¿›è¡Œç”Ÿæˆã€‚

<context>
{context}
</context>

ç”¨æˆ·çš„æé—®æ˜¯: {query}""")

# 2.æ„å»ºå¤§è¯­è¨€æ¨¡å‹
llm = ChatOpenAI(model="gpt-3.5-turbo-16k")

# 3.è¾“å‡ºè§£æå™¨
parser = StrOutputParser()

# 4.æ„å»ºé“¾
chain = RunnablePassthrough.assign(context=lambda x: retrieval(x["query"])) | prompt | llm | parser

# 5.è°ƒç”¨é“¾
content = chain.invoke({"query": "ä½ å¥½ï¼Œæˆ‘æ˜¯è°?"})

print(content)

```

:::

## ğŸŒµ æ€»ç»“

- `Prompt` : è¾“å…¥è®¾è®¡ï¼Œä¿è¯æ¨¡å‹æ–¹å‘æ­£ç¡®ï¼›
- `Model` : æ‰§è¡Œå¼•æ“ï¼Œè´Ÿè´£è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼›
- `OutputParser` : è¾“å‡ºæ²»ç†ï¼Œè®©ç»“æœå¯æ§ã€å¯ç”¨ï¼›
- `Runnable` : ç»Ÿä¸€æ¥å£ï¼Œè®©æ‰€æœ‰ç»„ä»¶èƒ½è¢«ç»„åˆï¼›
- `LCEL` : ç¼–æ’è¯­è¨€ï¼Œè®©é“¾æ¡æ„å»ºæ›´ç›´è§‚ï¼›
- `Passthrough` & `Parallel` : çµæ´»æ‰©å±•ï¼Œæ”¯æŒä¸Šä¸‹æ–‡æ³¨å…¥ä¸å¤šç»“æœå¹¶è¡Œã€‚
